{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ab15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61acee0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9836bf",
   "metadata": {},
   "source": [
    "### Function to save and load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449664ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(name, obj):\n",
    "    \"\"\"\n",
    "    Function to save an object as pickle file\n",
    "    \"\"\"\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_file(name):\n",
    "    \"\"\"\n",
    "    Function to load a pickle object\n",
    "    \"\"\"\n",
    "    return pickle.load(open(name, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430721a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aac2d5",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "tokens_path = \"Output/tokens.pkl\"\n",
    "file_path = \"Input/complaints.csv\"\n",
    "col_name = \"Consumer complaint narrative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7175fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd148f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6281c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25307207",
   "metadata": {},
   "source": [
    "### Drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in the specified column\n",
    "data.dropna(subset=[col_name], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd02f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the specified column from the DataFrame and assign it to the variable input_text\n",
    "input_text = data[col_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c1096",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a403a0a",
   "metadata": {},
   "source": [
    "### Convert text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a713509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data in the list to lowercase while displaying a progress bar\n",
    "input_text = [i.lower() for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb923d",
   "metadata": {},
   "source": [
    "### Remove punctuations except apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation characters (except apostrophes) from text in the list while displaying a progress bar\n",
    "input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i) for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509c438",
   "metadata": {},
   "source": [
    "### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits from text in the list while displaying a progress bar\n",
    "input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcb906",
   "metadata": {},
   "source": [
    "### Remove 'xxxx' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove consecutive instances of 'x' from text in the list while displaying a progress bar\n",
    "input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ba609",
   "metadata": {},
   "source": [
    "### Remove additional spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd85611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace multiple consecutive spaces with a single space in text within the list while displaying a progress bar\n",
    "input_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3eb14",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24308af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text within the first 100 elements of input_text and store the tokens in the tokens list while displaying a progress bar\n",
    "tokens = [word_tokenize(t) for t in tqdm(input_text[:100])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39be9f",
   "metadata": {},
   "source": [
    "### Save tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f07653",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(tokens_path, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51402e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a385d2",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "t = 1e-5\n",
    "context_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from Source.utils import save_file\n",
    "\n",
    "# Define the SkipGramDataset class\n",
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, input_data, context_window=5, out_path=\"Output\", t=1e-5, k=10):\n",
    "        # Initialize the dataset\n",
    "        self.k = k\n",
    "        self.context_window = context_window\n",
    "\n",
    "        # Count the frequency of words in the input data\n",
    "        print(\"Counting word tokens...\")\n",
    "        counter = Counter([t for d in tqdm(input_data) for t in d])\n",
    "        self.vocab_count = len(counter)\n",
    "        print(f\"Unique words in the corpus: {self.vocab_count}\")\n",
    "\n",
    "        # Create positive data samples for Skip-gram\n",
    "        print(\"Creating data samples...\")\n",
    "        self.samples = self.positive_samples(input_data)\n",
    "\n",
    "        # Create vocabulary mapping and sampling probabilities\n",
    "        word2idx = dict()\n",
    "        idx2word = dict()\n",
    "        sampling_prob = []\n",
    "        print(\"Generating vocabulary...\")\n",
    "        for i, c in enumerate(counter.most_common(len(counter))):\n",
    "            word2idx[c[0]] = i\n",
    "            idx2word[i] = c[0]\n",
    "            sampling_prob.append(c[1])\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "\n",
    "        # Calculate and normalize sampling probabilities\n",
    "        print(\"Calculating sampling probabilities...\")\n",
    "        sampling_prob = np.sqrt(t/np.array(sampling_prob))\n",
    "        sampling_prob = sampling_prob / np.sum(sampling_prob)\n",
    "        self.sampling_prob = sampling_prob\n",
    "\n",
    "        # Save vocabulary mapping to files\n",
    "        print(\"Saving files...\")\n",
    "        self.save_files(out_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate negative samples and prepare the dataset for training\n",
    "        neg_words = self.negative_samples()\n",
    "        center_word = self.word2idx[self.samples.loc[idx, \"center_word\"]]\n",
    "        context_word = self.word2idx[self.samples.loc[idx, \"context_word\"]]\n",
    "        return torch.tensor(center_word), torch.tensor([context_word]+neg_words)\n",
    "\n",
    "    def positive_samples(self, input_data):\n",
    "        # Create positive data samples by defining context windows\n",
    "        samples = []\n",
    "        cw = self.context_window\n",
    "        for data in tqdm(input_data):\n",
    "            text = [None] * cw + data + [None] * cw\n",
    "            for i in range(cw, len(text) - cw):\n",
    "                samples.append((text[i], text[i - cw:i] + text[i + 1: i + cw + 1]))\n",
    "        samples = pd.DataFrame(samples, columns=[\"center_word\", \"context_word\"])\n",
    "        samples = samples.explode(\"context_word\")\n",
    "        samples.dropna(inplace=True)\n",
    "        samples.reset_index(drop=True, inplace=True)\n",
    "        return samples\n",
    "\n",
    "    def negative_samples(self):\n",
    "        # Generate negative samples for Skip-gram training\n",
    "        neg_words = list(np.random.choice(np.arange(self.vocab_count), self.k, p=self.sampling_prob))\n",
    "        return neg_words\n",
    "\n",
    "    def save_files(self, out_path=\"Output\"):\n",
    "        # Save vocabulary mapping to files\n",
    "        save_file(os.path.join(out_path, \"word2idx.pkl\"), self.word2idx)\n",
    "        save_file(os.path.join(out_path, \"idx2word.pkl\"), self.idx2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57d120",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0d15b",
   "metadata": {},
   "source": [
    "# Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14303c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the SkipGram class\n",
    "class SkipGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, embedding_size=64):\n",
    "        # Initialize the SkipGram model\n",
    "        super(SkipGram, self).__init()\n",
    "\n",
    "        # Define the word embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_len, embedding_size)\n",
    "\n",
    "        # Initialize the weights matrix for Skip-gram\n",
    "        self.weights = torch.empty(embedding_size, vocab_len, requires_grad=True).type(torch.FloatTensor)\n",
    "        _ = torch.nn.init.normal_(self.weights)\n",
    "\n",
    "        # Define the output layer with LogSigmoid activation\n",
    "        self.out = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_word, context_words):\n",
    "        # Define the forward pass for Skip-gram\n",
    "        embeddings_ = self.embeddings(center_word)\n",
    "        weights_ = self.weights[:, context_words]\n",
    "        output = torch.einsum('bi,ibo->bo', embeddings_, weights_)\n",
    "        true_y = torch.zeros(output.shape[0], dtype=torch.int64)\n",
    "        return self.out(output), true_y\n",
    "\n",
    "    def save_files(self, out_path=\"Output\"):\n",
    "        # Save the model's embeddings and weights to files\n",
    "        save_file(os.path.join(out_path, \"emb.pkl\"), self.embeddings)\n",
    "        save_file(os.path.join(out_path, \"weights.pkl\"), self.weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743f47d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of negative samples for Skip-gram training\n",
    "k = 10\n",
    "\n",
    "# Learning rate for model training\n",
    "lr = 0.01\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# Batch size for training data\n",
    "batch_size = 128\n",
    "\n",
    "# Context window size for Skip-gram training\n",
    "context_window = 5\n",
    "\n",
    "# Output path for saving model and data files\n",
    "out_path = \"Output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a CUDA-compatible GPU is available; if not, use the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sg(dataloader, model, criterion, optimizer, device, num_epochs):\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize variables to track training progress\n",
    "    best_loss = 1e8  # A high initial value for tracking the best loss\n",
    "    patience = 0     # Counter for early stopping\n",
    "\n",
    "    # Loop over a specified number of training epochs\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = []  # List to store losses for each epoch\n",
    "        print(f\"Epoch {i+1} of {num_epochs}\")\n",
    "        \n",
    "        # Iterate over the data loader (batches of training data)\n",
    "        for center_word, context_words in tqdm(dataloader):\n",
    "            center_word = center_word.to(device)\n",
    "            context_words = context_words.to(device)\n",
    "            \n",
    "            # Forward pass: compute model output and loss\n",
    "            output, true_y = model(center_word, context_words)\n",
    "            loss = criterion(output, true_y)\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Append the loss for the current batch to the epoch_loss list\n",
    "            epoch_loss.append(loss.item())\n",
    "        \n",
    "        # Calculate the average loss for the current epoch\n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        \n",
    "        # Update best_loss if the current epoch's loss is better\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        \n",
    "        # Print the loss for the current epoch\n",
    "        print(f\"Loss: {epoch_loss}\")\n",
    "        \n",
    "        # Check for early stopping based on patience\n",
    "        if patience == 5:\n",
    "            print(\"Early stopping...\")\n",
    "    \n",
    "    # Save model files after training\n",
    "    model.save_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff902a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SkipGramDataset instance with the following parameters\n",
    "dataset = SkipGramDataset(input_data=tokens,  # Input data, typically tokenized text\n",
    "                          context_window=context_window,  # Size of the context window\n",
    "                          out_path=out_path,  # Output path for saving model files\n",
    "                          t=t,  # Threshold parameter for word sampling\n",
    "                          k=k)  # Number of negative samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch data loader for the SkipGramDataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset,  # The dataset to load\n",
    "                                         batch_size=batch_size,  # Batch size for training\n",
    "                                         shuffle=True,  # Shuffle the data in each epoch\n",
    "                                         drop_last=True)  # Drop the last batch if it's incomplete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SkipGram model with the specified vocabulary size and embedding size\n",
    "model = SkipGram(dataset.vocab_count, embedding_size=embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27590d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss criterion\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Initialize the optimizer with model parameters and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12580818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Skip-gram model\n",
    "train_sg(dataloader, model, criterion, optimizer, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f91c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d5dd1",
   "metadata": {},
   "source": [
    "# Using embedings to get word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word-to-index dictionary from a file\n",
    "word2idx = load_file(\"Output/word2idx.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f220a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx[\"payments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945905c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_file(\"Output/emb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580df0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings(torch.tensor(83))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564248c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed477fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
